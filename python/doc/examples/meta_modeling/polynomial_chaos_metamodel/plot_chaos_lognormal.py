"""
Pitfalls in  polynomial chaos expansion due to the input distribution
=====================================================================
"""

# %%
#
# In this example, we construct a polynomial
# chaos expansion (refer to :ref:`functional_chaos` for more details)
# using a countable family of polynomials orthonormal with respect to an input distribution :math:`\mu`
# such that
# the functional space generated by this orthonormal polynomial family
# is **not dense** in the functional Hilbert space :math:`L^2(\mu)`
# of square-integrable functions with respect to :math:`\mu`.
#
# Therefore, there exist functions in :math:`L^2(\mu)` that are not equal to
# their orthogonal projection onto the closure of the polynomial space.
# Moreover, any approximation obtained by truncating this projection
# may lead to a low-quality surrogate model.
#
# This situation occurs, for instance, when :math:`\mu`
# is not characterized by the infinite sequence of its moments.
# The :class:`~openturns.LogNormal` distribution is one such example.
#
# We also show that numerical cancellation issues may arise
# when using a large number of polynomials in the surrogate model.
#
# We consider the model :math:`\model:  \Rset* \rightarrow \Rset` defined by:
#
# .. math::
#    \model(x) = \dfrac{1}{x}
#
# Let the random variable :math:`X` follow a :class:`~openturns.LogNormal`
# distribution whose PDF is denoted by :math:`\mu`.
# This distribution is not characterized by the infinite sequence of its moments.
#
# We proceed as follows:
#
#  - **Introduction:** we build the family of polynomials
#    orthonormal with respect to the :class:`~openturns.LogNormal` distribution up to high degrees,
#  - **Case 1:** we create a polynomial chaos surrogate model of :math:`\model`
#    using the polynomials orthonormal with respect to the LogNormal distribution,
#  - **Case 2:** we show how to obtain a higher-quality surrogate model
#    using another polynomial family: the polynomials orthonormal
#    with respect to the :class:`~openturns.Normal` distribution.

# %%
import openturns as ot
import openturns.viewer as otv
from math import sqrt
# sphinx_gallery_thumbnail_number = 7

# %%
# Introduction: orthonormal polynomial family w.r.t. the LogNormal distribution
# -----------------------------------------------------------------------------
#
# In this section, we build the family of polynomials orthonormal with respect to the LogNormal distribution
# using the :class:`~openturns.AdaptiveStieltjesAlgorithm`.
#
# We build the polynomials up to degree 40; they are defined by their three-term recurrence coefficients.
dist_X = ot.LogNormal()
lower_Bound = dist_X.computeQuantile(0.01, False)
upper_Bound = dist_X.computeQuantile(0.01, True)
degreeMax = 30

as_Algo = ot.AdaptiveStieltjesAlgorithm(dist_X)
sample_Coeff = ot.Sample(0, 3)
for i in range(degreeMax):
    coeff = as_Algo.getRecurrenceCoefficients(i)
    sample_Coeff.add(coeff)

# %%
# We build a set of polynomials with increasing degrees.
# For each polynomial, we display its value at :math:`x = 0`.
#
# We anticipate that the vicinity of :math:`x = 0` will cause difficulties
# for the surrogate model of :math:`\model`, since
# :math:`\lim_{x \rightarrow 0} \model(x) = +\infty`.
# Given that the polynomials take *reasonable values* in the neighborhood of :math:`x = 0`,
# we expect the polynomial surrogate model to require
# very large linear coefficients in order to reproduce high values near :math:`x = 0`,
# which may lead to numerical cancellation issues.
#
# We explore polynomial degrees up to 40.
deg_list = [5, 10, 20, 30, 40]
g = ot.Graph('Polynomials orthonormal  with respect to the LogNormal distribution', 'x', 'y', True)
leg = ot.Description(0)
for indice, deg in enumerate(deg_list):
    coeff_Poly_Deg = sample_Coeff[0:deg]
    poly_Deg = ot.OrthogonalUniVariatePolynomial(coeff_Poly_Deg)
    g.add(poly_Deg.draw(lower_Bound[0], upper_Bound[0], 1024))
    leg.add(r'$d = $' + str(deg))
    print('degree = ', deg, 'polynomial(0) = ', poly_Deg(0))

g.setLegends(leg)
g.setLegendPosition('topleft')
view = otv.View(g)

# %%
# We observe that the polynomials up to degree 40:
#
# - are computed in a stable manner, as indicated by their smooth graphs,
# - take reasonable values on the interval :math:`[0, 25]`.

# %%
# Case 1: PCE using orthonormal polynomial family w.r.t. the LogNormal distribution
# ---------------------------------------------------------------------------------
#
# We define :math:`\model`, the input random variable :math:`X` and the output random variable :math:`Y = \model(X)`.
input_Dim = dist_X.getDimension()
input_RV = ot.RandomVector(dist_X)

g = ot.SymbolicFunction('x', '1/(x)')

output_RV = ot.CompositeRandomVector(g, input_RV)

# %%
# We create a training sample.
sample_Size = 1000
input_Train = dist_X.getSample(sample_Size)
output_Train = g(input_Train)
print('max sample = ', input_Train.getMax()[0])


# %%
# We now construct the polynomial chaos surrogate model :math:`\metaModel`
# using the polynomials up to degree 5. Note that the coefficients in the linear combination of
# the polynomials are large.
basis_LN = ot.OrthogonalProductPolynomialFactory([ot.StandardDistributionPolynomialFactory(ot.LogNormal())])
basis_Size_LN = 6
chaos_Algo_LN = ot.LeastSquaresExpansion(input_Train, output_Train, dist_X, basis_LN, basis_Size_LN)
chaos_Algo_LN.run()
result_LN = chaos_Algo_LN.getResult()
meta_Model_LN = result_LN.getMetaModel()
print('meta_Model_LN = ', meta_Model_LN)
print('coeff = ', result_LN.getCoefficients())

# %%
# We assess the quality of the surrogate model using the :class:`~openturns.MetaModelValidation` class.
# We compute the :math:`R^2` score, which is very low.
meta_Model_Predictions_LN = meta_Model_LN(input_Train)
valid_Meta_Model_LN = ot.MetaModelValidation(output_Train, meta_Model_Predictions_LN)
r2_Score_LN = valid_Meta_Model_LN.computeR2Score()[0]
r2_Score_LN

graph_Valid_LN = valid_Meta_Model_LN.drawValidation()
graph_Valid_LN.setTitle(r"$R^2=$%.2f%%, LogNormal polyn. family, $d \leq $" % (r2_Score_LN * 100) + str(basis_Size_LN - 1))
view = otv.View(graph_Valid_LN)

# %%
# Here, we plot the model and the surrogate model.
# The differences in the region where :math:`x \rightarrow +\infty` are not significant,
# since the LogNormal distribution assigns negligible probability mass to this region.
# Thus, even if the surrogate model deviates from the true model for :math:`x \rightarrow +\infty`,
# it does not reduce the overall quality of the surrogate model.
#
# In contrast, differences near :math:`x = 0` have a major impact on the quality of the surrogate model.
graph_LN = g.draw(lower_Bound, upper_Bound, [251])
graph_LN.add(meta_Model_LN.draw(lower_Bound, upper_Bound, [251]))
graph_LN.setLegends(['model', 'surrogate model (LN)'])
graph_LN.setLegendPosition('topright')
graph_LN.setTitle(r'Chaos expansion using Lognormal orthonormal polynomials family, $d \leq $' + str(basis_Size_LN - 1))
graph_LN.setXTitle('x')
graph_LN.setYTitle('y')
view = otv.View(graph_LN)

# %%
# In conclusion, the surrogate model is a low-quality metamodel.
#
# We then increase the number of polynomials used in the surrogate model.
# We observe that the surrogate model does not converge to the true model, even for high polynomial degrees.
# For each surrogate model, we display the coefficients of the linear combination.
# It is evident that the coefficients become very large.
#
# Finally, we save the surrogate model so that it can be easily reused later.
basis_Size_List = [16, 21]
leg = graph_LN.getLegends()
leg[1] = r'$d \leq $' + str(basis_Size_LN)
meta_Model_LN_List = list()
for basis_Size in basis_Size_List:
    chaos_Algo_LN = ot.LeastSquaresExpansion(input_Train, output_Train, dist_X, basis_LN, basis_Size)
    chaos_Algo_LN.run()
    meta_Model_LN = chaos_Algo_LN.getResult().getMetaModel()
    meta_Model_LN_List.append(meta_Model_LN)
    graph_LN.add(meta_Model_LN.draw(lower_Bound, upper_Bound, [1024]))
    leg.add(r'$d \leq $' + str(basis_Size - 1))
    result_LN = chaos_Algo_LN.getResult()
    meta_Model_LN = result_LN.getMetaModel()
    print('degree = ', basis_Size - 1, 'coefficients of the meta Model (LN) = ')
    print(result_LN.getCoefficients())

graph_LN.setLegends(leg)
view = otv.View(graph_LN)


# %%
# We note that the graph of the surrogate model is not smooth, even for degree 20,
# although the surrogate model takes reasonable values on :math:`[0, 25]`.
# These oscillations are caused by the coefficients of the linear combination,
# which have become very large, as anticipated earlier,
# leading to numerical cancellation issues in the evaluations of the surrogate model.
#
# We now compute the :math:`L^2(\mu)`-error between the surrogate model and the true model, defined by
# :math:`\|g-\tilde{g}\|_{L^2(\mu)} = \int_{\Rset} (\model(x)-\tilde{\model}(x))^2 \mu(x) dx`.
sample_In = dist_X.getSample(100000)
sample_Model = g(sample_In)
for i in range(len(basis_Size_List)):
    sample_Predic = meta_Model_LN_List[i](sample_In)
    square_L2_Norm = (sample_Predic - sample_Model).computeRawMoment(2)[0]
    print('degree = ', basis_Size_List[i] - 1, 'L2 norme = ', sqrt(square_L2_Norm))

# %%
# We observe that the :math:`L^2(\mu)`-error increases,
# indicating that the surrogate model does not converge to the true model
# as the number of polynomials used in the surrogate model increases.

# %%
# Case 2: PCE using orthonormal polynomial family w.r.t. another distribution
# ---------------------------------------------------------------------------
#
# We choose to project the model :math:`\model` onto the functional space
# generated by the family of polynomials orthonormal with respect to the
# :class:`~openturns.Normal` distribution. The Normal distribution is
# characterized by its infinite sequence of moments.
#
# We construct the surrogate model as before, using only the polynomials up to degree 5.
basis_N = ot.OrthogonalProductPolynomialFactory([ot.StandardDistributionPolynomialFactory(ot.Normal())])
basis_Size_N = 6
chaos_Algo_N = ot.LeastSquaresExpansion(input_Train, output_Train, dist_X, basis_N, basis_Size_N)

chaos_Algo_N.run()
meta_Model_N = chaos_Algo_N.getResult().getMetaModel()

# %%
# We assess the quality of the surrogate model and compute the :math:`R^2` score, which is very high.
meta_Model_Predictions_N = meta_Model_N(input_Train)
valid_Meta_Model_N = ot.MetaModelValidation(output_Train, meta_Model_Predictions_N)
r2_Score_N = valid_Meta_Model_N.computeR2Score()[0]
r2_Score_N

graph_Valid_N = valid_Meta_Model_N.drawValidation()
graph_Valid_N.setTitle(r"$R^2=$%.2f%%, Normal polyn. family, $d \leq $" % (r2_Score_N * 100) + str(basis_Size_N - 1))
view = otv.View(graph_Valid_N)


# %%
# Here, we plot the model and the surrogate model.
# No differences are observed between the model and the surrogate model
# across the entire interval :math:`[0, 25]`.
graph_N = g.draw(lower_Bound, upper_Bound, [251])
graph_N.add(meta_Model_N.draw(lower_Bound, upper_Bound, [251]))
graph_N.setLegends(['model', 'surrogate model (N)'])
graph_N.setLegendPosition('topright')
graph_N.setTitle(r'Chaos expansion using Normal orthonormal polynomials family, $d \leq$' + str(basis_Size_N - 1))
graph_N.setXTitle('x')
graph_N.setYTitle('y')
view = otv.View(graph_N)

# %%
# We compute the :math:`L^2`-error between the surrogate model and the true model,
# which is very low.
sample_Predic_N = meta_Model_N(sample_In)
square_L2_N_Norm = (sample_Predic_N - sample_Model).computeRawMoment(2)[0]
print('degree = ', basis_Size_N - 1, 'L2 norme = ', sqrt(square_L2_N_Norm))

# %%
# We note that there are no longer any cancellation problems in the evaluation of the surrogate model.
# Indeed, if we denote by :math:`T` the isoprobabilistic transformation that maps the distribution of
# :math:`X` onto the Normal distribution of :math:`Z`, we have :math:`X = T(Z) = e^Z`.
# Thus, the polynomial chaos expansion is constructed for :math:`h(z) = g \circ T(z) = e^{-z}`.
# This function is analytical with an infinite radius of convergence, so its polynomial
# approximation converges very quickly and does not require many polynomials.
#
# Finally, the resulting surrogate model is given by :math:`\metaModel = \tilde{h} \circ T^{-1}`.
#
# Here, we facilitate the comparison between the surrogate model constructed from the polynomial family orthonormal
# with respect to the LogNormal distribution up to degree 20,
# and the surrogate model constructed from the polynomial family orthonormal
# with respect to the Normal distribution up to degree 5.
graph_Comp = ot.Graph(r'Chaos expansion of $f: x \rightarrow 1/(1+x)$,  $X \sim LogNormal$', 'x', 'y', True)
graph_Comp = g.draw(lower_Bound, upper_Bound, [251])
graph_Comp.add(meta_Model_N.draw(lower_Bound, upper_Bound, [251]))
graph_Comp.add(meta_Model_LN_List[-1].draw(lower_Bound, upper_Bound, [251]))
graph_Comp.setLegends(['model', r'polyn. $\perp$ Normal, $d \leq$ ' + str(basis_Size_N - 1), r'polyn. $\perp$ LogNormal, $d \leq $ ' + str(basis_Size_List[-1] - 1)])
graph_Comp.setLegendPosition('topright')
graph_Comp.setTitle(r'Chaos expansion of $f: x \rightarrow 1/(1+x)$,  $X \sim LogNormal$')
graph_Comp.setXTitle('x')
graph_Comp.setYTitle('y')
view = otv.View(graph_Comp)

# %%
# We display all figures.
otv.View.ShowAll()
