"""
Pitfalls in  polynomial chaos expansion due to the input distribution
"""

# %%
#
# In this example, we create a polynomial chaos expansion
# (refer to :ref:`functional_chaos` for more details)
# using the polynomial basis orthogonal
# with respect to a input distribution which is not characterised by the infinite
# sequence of its moments.
# It implies that the functional space generated by the polynomials family
# is not dense in the functional Hilbert space of square-integrable functions with respect to the
# input distribution.
# In other words, the model under consideration might not be equal to its projection
# based on the orthonormal polynomials family. Therefore, any approximation consisting
# of truncating this projection might result in a poor-quality meta-model.
#
# We consider the model :math:`\model:  \Rset \rightarrow \Rset` defined by:
#
# .. math::
#    \model(x) = \dfrac{1}{x}
#
# and the random variable :math:`X` following a :class:`~openturns.LogNormal`
# distribution which PDF is denoted by :math:`\mu`. This distribution is not characterised by the infinite sequence
# of its moments.
#
# We proceed as follows:
#  - Introduction: we build the  polynomial
#    basis orthonormal with respect to the :class:`~openturns.LogNormal` distribution up to high degrees,
#  - Case 1: we create a polynomial chaos meta model of :math:`\model` using the polynomial
#    basis orthonormal with respect to the LogNormal distribution,
#  - Case 2: we show how to get a higher-quality meta model using another polynomial
#    orthonormal basis: the one associated to the  :class:`~openturns.Normal` distribution.
#

# %%
import openturns as ot
import openturns.viewer as otv
from math import sqrt
# sphinx_gallery_thumbnail_number = 7

# %%
# Introduction: polynomial basis orthonormal with respect to the LogNormal distribution
# -------------------------------------------------------------------------------------
#
# In that section, we build the polynomial basis orthonormal with respect to the LogNormal distribution thanks to
# the :class:`~openturns.AdaptiveStieltjesAlgorithm`.
#
# We build the polynomials up to the degree 40: they are defined by their three-term-recurrence coefficients.
dist_X = ot.LogNormal()
lower_Bound = dist_X.computeQuantile(0.01, False)
upper_Bound = dist_X.computeQuantile(0.01, True)
degreeMax = 40

as_Algo = ot.AdaptiveStieltjesAlgorithm(dist_X)
sample_Coeff = ot.Sample(0, 3)
for i in range(degreeMax):
    coeff = as_Algo.getRecurrenceCoefficients(i)
    sample_Coeff.add(coeff)

# %%
# We build some polynomials with increasing degrees. For each of them, we display its value at :math:`x=0`.
# We guess that the  vicinity of :math:`x=0` will pose a problem to the meta model: as a matter of fact,
# the model is such that :math:`\lim_{x \rightarrow 0} \model(x) = +\infty`. Thus, the polynomials meta model will have
# some gigantic linear coefficients to get high values around :math:`x=0`.
#
# On the contrary, the vicinity of :math:`+\infty` will not pose a problem because the LogNormal distribution
# gives no probabilistic mass to this region. Thus, even if the meta model is far from the model for :math:`x \rightarrow +\infty`,
# it does not decrease the quality of the meta model.
#
# We explore the degrees up to 40.
deg_list = [5, 10, 20, 30, 40]
g = ot.Graph('Polynomials orthonormal  with respect to the LogNormal distribution', 'x', 'y', True)
leg = ot.Description(0)
for indice, deg in enumerate(deg_list):
    coeff_Poly_Deg = sample_Coeff[0:deg]
    poly_Deg = ot.OrthogonalUniVariatePolynomial(coeff_Poly_Deg)
    g.add(poly_Deg.draw(lower_Bound[0], upper_Bound[0], 1024))
    leg.add(r'$d = $' + str(deg))
    print('degree = ', deg, 'polynomial(0) = ', poly_Deg(0))

g.setLegends(leg)
g.setLegendPosition('topleft')
view = otv.View(g)

# %%
# We can see that the polynomials up to degree 40 are:
# - computed in a stable manner since their graph is smooth,
# - they take reasonable values on the interval :math:`[0,25]`.
#

# %%
# Case 1: Using the polynomials basis orthonormal with respect to the LogNormal distribution
# ------------------------------------------------------------------------------------------
#
# Define :math:`\model`, the input random variable :math:`X` and the output random variable :math:`Y = f(X)`.
input_Dim = dist_X.getDimension()
input_RV = ot.RandomVector(dist_X)

g = ot.SymbolicFunction('x', '1/(x)')

output_RV = ot.CompositeRandomVector(g, input_RV)

# %%
# Create a training sample.
sample_Size = 1000
input_Train = dist_X.getSample(sample_Size)
output_Train = g(input_Train)
print('max sample = ', input_Train.getMax()[0])


# %%
# Now we create the polynomials chaos meta model :math:`\tilde{g}`, using the polynomials up to degree 5.
# Note that the coefficients of the linear combination of the polynomials are high.
basis_LN = ot.OrthogonalProductPolynomialFactory([ot.StandardDistributionPolynomialFactory(ot.LogNormal())])
basis_Size_LN = 6
chaos_Algo_LN = ot.LeastSquaresExpansion(input_Train, output_Train, dist_X, basis_LN, basis_Size_LN)
chaos_Algo_LN.run()
result_LN = chaos_Algo_LN.getResult()
meta_Model_LN = result_LN.getMetaModel()
print('meta_Model_LN = ', meta_Model_LN)
print('coeff = ', result_LN.getCoefficients())

# %%
# We check the quality of the meta model using the :class:`~openturns.MetaModelValidation` class:
# We compute the :math:`R^2` score, which is really bad.
meta_Model_Predictions_LN = meta_Model_LN(input_Train)
valid_Meta_Model_LN = ot.MetaModelValidation(output_Train, meta_Model_Predictions_LN)
r2_Score_LN = valid_Meta_Model_LN.computeR2Score()[0]
r2_Score_LN

graph_Valid_LN = valid_Meta_Model_LN.drawValidation()
graph_Valid_LN.setTitle(r"$R^2=$%.2f%%, LogNormal polyn. family, $d \leq $" % (r2_Score_LN * 100) + str(basis_Size_LN - 1))
view = otv.View(graph_Valid_LN)

# %%
# Here, we draw the model and the meta model. Do not forget that the differences in the region where
# :math:`x \rightarrow +\infty` have no importance, given the input distribution, compared to the differences
# in the vicinity of :math:`x=0`.
graph_LN = g.draw(lower_Bound, upper_Bound, [251])
graph_LN.add(meta_Model_LN.draw(lower_Bound, upper_Bound, [251]))
graph_LN.setLegends(['model', 'meta model (LN)'])
graph_LN.setLegendPosition('topright')
graph_LN.setTitle(r'Chaos expansion using Lognormal orthonormal polynomials family, $d \leq $' + str(basis_Size_LN - 1))
graph_LN.setXTitle('x')
graph_LN.setYTitle('y')
view = otv.View(graph_LN)

# %%
# In conclusion, the meta model is a poor-quality meta-model.
#
# Now, we increase the number of polynomials used
# in the meta model. We can see that the meta model does not converge to the model even with high polynomials degrees.
#
# We save the meta model so that it can be easily reused later.
#
# For each  meta model, we display the coefficients of the linear combination.  We can see that
# the coefficients become huge.
basis_Size_List = [16, 21]
leg = graph_LN.getLegends()
leg[1] = r'$d \leq $' + str(basis_Size_LN)
meta_Model_LN_List = list()
for basis_Size in basis_Size_List:
    chaos_Algo_LN = ot.LeastSquaresExpansion(input_Train, output_Train, dist_X, basis_LN, basis_Size)
    chaos_Algo_LN.run()
    meta_Model_LN = chaos_Algo_LN.getResult().getMetaModel()
    meta_Model_LN_List.append(meta_Model_LN)
    graph_LN.add(meta_Model_LN.draw(lower_Bound, upper_Bound, [1024]))
    leg.add(r'$d \leq $' + str(basis_Size - 1))
    result_LN = chaos_Algo_LN.getResult()
    meta_Model_LN = result_LN.getMetaModel()
    print('degree = ', basis_Size - 1, 'coefficients of the meta Model (LN) = ')
    print(result_LN.getCoefficients())

graph_LN.setLegends(leg)
view = otv.View(graph_LN)


# %%
# We note that the graph of the meta model is not smooth even for degree 20. However, the
# meta model takes reasonable values on :math:`[0, 25]`. Thess oscillations are due to the coefficients
# of the linear combination which have become gigantic, as we explained earlier,
# leading to cancellation problems.

# %%
# We compute here the :math:`L^2`-error between the meta model and the model, defined by
# :math:`\|g-\tilde{g}\|_{L^2(\mu)} = \int_{\Rset} (\model(x)-\tilde{\model}(x))^2 \mu(x) dx`.
sample_In = dist_X.getSample(100000)
sample_Model = g(sample_In)
for i in range(len(basis_Size_List)):
    sample_Predic = meta_Model_LN_List[i](sample_In)
    square_L2_Norm = (sample_Predic - sample_Model).computeRawMoment(2)[0]
    print('degree = ', basis_Size_List[i] - 1, 'L2 norme = ', sqrt(square_L2_Norm))

# %%
# We note that the  :math:`L^2`-error increases, which shows that the meta model does not converge towards the model
# when we increase the number of polynomials used in the meta model.

# %%
# Case 2: Using another orthonormal polynomials basis
# ---------------------------------------------------
#
# We decide to project the model :math:`\model` on the functional space generated by the polynomials family orthonormal
# with respect to the :class:`~openturns.Normal` distribution which is characterized by  the infinite
# sequence of its moments.
#
# We build the meta model as previously, using only the polynomials up to degree 5.
basis_N = ot.OrthogonalProductPolynomialFactory([ot.StandardDistributionPolynomialFactory(ot.Normal())])
basis_Size_N = 6
chaos_Algo_N = ot.LeastSquaresExpansion(input_Train, output_Train, dist_X, basis_N, basis_Size_N)

chaos_Algo_N.run()
meta_Model_N = chaos_Algo_N.getResult().getMetaModel()

# %%
# We check the quality of the meta model and we compute the :math:`R^2` score which is very good.
meta_Model_Predictions_N = meta_Model_N(input_Train)
valid_Meta_Model_N = ot.MetaModelValidation(output_Train, meta_Model_Predictions_N)
r2_Score_N = valid_Meta_Model_N.computeR2Score()[0]
r2_Score_N

graph_Valid_N = valid_Meta_Model_N.drawValidation()
graph_Valid_N.setTitle(r"$R^2=$%.2f%%, Normal polyn. family, $d \leq $" % (r2_Score_N * 100) + str(basis_Size_N - 1))
view = otv.View(graph_Valid_N)


# %%
# Here, we draw the model and the meta model. We do not see any differences between the model and the meta model across
# all the intervale :math:`[0, 25]`.
graph_N = g.draw(lower_Bound, upper_Bound, [251])
graph_N.add(meta_Model_N.draw(lower_Bound, upper_Bound, [251]))
graph_N.setLegends(['model', 'meta model (N)'])
graph_N.setLegendPosition('topright')
graph_N.setTitle(r'Chaos expansion using Normal orthonormal polynomials family, $d \leq$' + str(basis_Size_N - 1))
graph_N.setXTitle('x')
graph_N.setYTitle('y')
view = otv.View(graph_N)

# %%
# We compute the :math:`L^2`-error between the meta model and the model: it is very good!
sample_Predic_N = meta_Model_N(sample_In)
square_L2_N_Norm = (sample_Predic_N - sample_Model).computeRawMoment(2)[0]
print('degree = ', basis_Size_N - 1, 'L2 norme = ', sqrt(square_L2_N_Norm))

# %%
# Here we facilitate the comparison between the meta model built from the polynomials family orthonormal with respect to the
# LogNormal distribution up to degree 20 and the meta model built from the polynomials family orthonormal with respect to the
# Normal distribution up to degree 5.
#
graph_Comp = ot.Graph(r'Chaos expansion of $f: x \rightarrow 1/(1+x)$,  $X \sim LogNormal$', 'x', 'y', True)
graph_Comp = g.draw(lower_Bound, upper_Bound, [251])
graph_Comp.add(meta_Model_N.draw(lower_Bound, upper_Bound, [251]))
graph_Comp.add(meta_Model_LN_List[-1].draw(lower_Bound, upper_Bound, [251]))
graph_Comp.setLegends(['model', r'polyn. $\perp$ Normal, $d \leq$ ' + str(basis_Size_N - 1), r'polyn. $\perp$ LogNormal, $d \leq $ ' + str(basis_Size_List[-1] - 1)])
graph_Comp.setLegendPosition('topright')
graph_Comp.setTitle(r'Chaos expansion of $f: x \rightarrow 1/(1+x)$,  $X \sim LogNormal$')
graph_Comp.setXTitle('x')
graph_Comp.setYTitle('y')
view = otv.View(graph_Comp)

# %%
# Display all figures
otv.View.ShowAll()
