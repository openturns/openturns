%feature("docstring") OT::LinearModelAnalysis
"Analyse a linear model.

Parameters
----------
linearModelResult : :class:`~openturns.LinearModelResult`
    A linear model result.

See Also
--------
LinearModelResult

Notes
-----
This class relies on a linear model result structure and analyses the results.

By default, on graphs, labels of the 3 most significant points are displayed.
This number can be changed by modifying the ResourceMap key
(``LinearModelAnalysis-IdentifiersNumber``).

The class has a pretty-print method which is triggered by
the `print()` function.
This prints the following results, where we focus on the properties
of a satisfactory regression model.

- Each row of the table of coefficients tests if one single coefficient is zero.
  For a single coefficient, if the p-value of the T-test is close to zero,
  we can reject the hypothesis that this coefficient is zero.
- The R2 score measures how the predicted output values are close to the
  observed values.
  If the R2 is close to 1 (e.g. larger than 0.95), then the predictions are
  accurate on average.
  Furthermore, the adjusted R2 value takes into account the data set
  size and the number of hyperparameters.
- The F-test tests if all the coefficients are simultaneously zero.
  If the p-value is close to zero, then we can reject this hypothesis:
  there is at least one nonzero coefficient.
- The normality test checks that the residuals have a normal distribution.
  The normality assumption can be accepted (or, more precisely, cannot be
  rejected) if the p-value is larger than a threshold (e.g. 0.05).

The basics on regression theory are presented in :ref:`regression_analysis`.
The goodness of fit tests for normality are presented in :ref:`graphical_fitting_test`, 
:ref:`chi2_fitting_test`, :ref:`kolmogorov_smirnov_test`, :ref:`cramer_vonmises_test`  and
:ref:`anderson_darling_test`.

Examples
--------
>>> import openturns as ot
>>> ot.RandomGenerator.SetSeed(0)
>>> distribution = ot.Normal()
>>> Xsample = distribution.getSample(30)
>>> func = ot.SymbolicFunction(['x'], ['2 * x + 1'])
>>> Ysample = func(Xsample) + ot.Normal().getSample(30)
>>> algo = ot.LinearModelAlgorithm(Ysample, Xsample)
>>> result = algo.getResult()
>>> analysis = ot.LinearModelAnalysis(result)
>>> # print(analysis)  # Pretty-print"

// ---------------------------------------------------------------------

%feature("docstring") OT::LinearModelAnalysis::getLinearModelResult
"Accessor to the linear model result.

Returns
-------
linearModelResult : :class:`~openturns.LinearModelResult`
    The  linear model result which has been passed to the constructor."

// ---------------------------------------------------------------------

%feature("docstring") OT::LinearModelAnalysis::getCoefficientsTScores
"Accessor to the coefficients of linear expansion over their standard error.

Returns
-------
tScores : :class:`~openturns.Point`
   The Student score of each coefficient estimate :math:`\hat{\vect{a}}`.

Notes
-----
For each coefficient estimate :math:`\hat{\vect{a}}`, the Student score :math:`T_k` is computed as: 

.. math::
   :label: tScores

   T_k = \dfrac{\hat{a}_k}{\sigma(a_k)}

where :math:`\sigma(a_k)` is the standard deviation of the distribution of the estimator :math:`\hat{a}_k`
defined in :eq:`std_dev_estim`.
"

// ---------------------------------------------------------------------

%feature("docstring") OT::LinearModelAnalysis::getCoefficientsPValues
"Accessor to the coefficients of the p values.

Returns
-------
pValues : :class:`~openturns.Point`
    Student P-values of the coefficient estimates.

Notes
-----
The p-values of each coefficient estimate is computed from the t-scores defined in :eq:`tScores` with
respect to the :class:`~openturns.Student` distribution
with :math:`dof` degrees of freedom defined in :eq:`dofWithBase` or :eq:`dofNoBase`.

These p-values are used under the Gaussian assumption of the error :math:`\varepsilon`. They test
if each coefficient is significantly different from zero. If the residuals are not Gaussian, this test is not appropriate and should not be used.
"
// ---------------------------------------------------------------------

%feature("docstring") OT::LinearModelAnalysis::getCoefficientsConfidenceInterval
"Accessor to the confidence interval of level :math:`\alpha` for the coefficients
of the linear expansion.

Parameters
----------
alpÄ¥a : float, :math:`0 \leq \alpha \leq 1`
    The confidence level :math:`\alpha`.

Returns
-------
confidenceInterval : :class:`~openturns.Interval`
    The confidence interval.

Notes
-----
The confidence interval of the coefficient :math:`a_k` of level :math:`\alpha` is defined by:

.. math::
   \left[\hat{a}_k \pm \sqrt{\left((\mat{\Psi}^t\mat{\Psi})^{-1} \right)_{k+1, k+1}}\hat{\sigma} t_{(1+\alpha)/2}\right]

where:

- :math:`t_{(1+\alpha)/2}` is the quantile of order :math:`(1+\alpha)/2` of the Student(:math:`dof`) distribution,
- with :math:`dof` the degrees of freedom defined in :eq:`dofNoBase` or :eq:`dofWithBase`,
- :math:`\mat{\Psi}` the design matrix defined in :eq:`designMatDef` or :eq:`designMatDef2`.

The interval returned is multivariate and contains the intervals of all the coefficients.
"

// ---------------------------------------------------------------------

%feature("docstring") OT::LinearModelAnalysis::getFisherScore
"Accessor to the Fisher statistics.

Returns
-------
fisherScore : float
    The Fisher score of the model.

Notes
-----
The Fisher score is computed as follows.

Let be :math:`dofM` the degrees of freedom of the model, equal to the number of coefficients to estimate (:math:`p+1` of :math:`p'`). If the basis contains an intercept, then we subtract 1 from :math:`dofM`.

Let :math:`dof` be the degrees of freedom defined in :eq:`dofNoBase` or :eq:`dofWithBase`.

Let *SSR* be the Sum of Squared Residuals (sometimes called *SSE* as Sum of Squared Errors) defined by: 

.. math::
   SSR = \sum_{i=1}^\sampleSize \varepsilon_i^2

Let *SST* be the Sum of Squared Total defined by: 

.. math::
   SST = \left| \begin{array}{ll}
       \sum_{i=1}^\sampleSize (Y_i - \bar{Y}_i)^2 & \mbox{if the basis contains an intercept} \\ 
       \sum_{i=1}^\sampleSize Y_i^2               & \mbox{if not.}
                \end{array}
         \right.

where :math:`\bar{Y}_i = \dfrac{1}{\sampleSize} Y_i`.

We denote by *SSM* the Sum of Squared Model defined by:

.. math::
   SSM = SST - SSR

Then, the Fisher score :math:`F` is defined by:

.. math::
   :label: FisherScore

   F = \dfrac{SSM/dofM}{SSE/dof}
"

// ---------------------------------------------------------------------

%feature("docstring") OT::LinearModelAnalysis::getFisherPValue
"Accessor to the Fisher p value.

Returns
-------
fisherPValue : float
    Fisher P-value of the model estimate.

Notes
-----
The Fisher p-value is computed from the Fisher score defined in :eq:`FisherScore` with
respect to the :class:`~openturns.FisherSnedecor` distribution parameterized by :math:`(dofM, dof)` where:

- :math:`dofM` is the degrees of freedom of the model (equal to the number of coefficients to estimate (:math:`p+1` of :math:`p'`). If the basis contains an intercept, then we subtract 1 from :math:`dofM`.
- :math:`dof` is the degrees of freedom defined in :eq:`dofNoBase` or :eq:`dofWithBase`.

This p-value is used under the Gaussian assumption of the error :math:`\varepsilon`. It tests
if all the coefficients are statistically useful to the model. If the residuals are not Gaussian, this test is not appropriate and should not be used."

// ---------------------------------------------------------------------

%feature("docstring") OT::LinearModelAnalysis::getResidualsStandardError
"Accessor to the standard error of the residuals.

Returns
-------
stdError : float
    The residuals standard deviation estimate.

Notes
-----
The standard error is also called the *root mean squared error* or the *standard error of regression*. It is
the residual standard deviation :math:`\hat{\sigma}` defined in :eq:`estimSigma2Noise` which is the unbiaised
residuals variance.
"

// ---------------------------------------------------------------------

%feature("docstring") OT::LinearModelAnalysis::getNormalityTestResultChiSquared
"Performs Chi-Square test.
The statistical test checks the Gaussian assumption of the residuals.

Returns
-------
testResult : :class:`~openturns.TestResult`
    Test result class.

Notes
-----
The Chi-Square test is a goodness of fit test which objective is to check the
normality assumption (null hypothesis) of residuals (and thus the model).

Usually, Chi-Square test applies for discrete distributions. Here we rely on
the :meth:`~openturns.FittingTest.ChiSquared` to check the normality.
"

// ---------------------------------------------------------------------

%feature("docstring") OT::LinearModelAnalysis::getNormalityTestResultKolmogorovSmirnov
"Performs Kolmogorov test.

Performs Kolmogorov test to check Gaussian assumption of the residuals.

Returns
-------
testResult : :class:`~openturns.TestResult`
    Test result class.

Notes
-----
We check that the residual is Gaussian thanks to :meth:`~openturns.FittingTest.Kolmogorov`."

// ---------------------------------------------------------------------

%feature("docstring") OT::LinearModelAnalysis::getNormalityTestResultAndersonDarling
"Performs Anderson-Darling test.
The statistical test checks the Gaussian assumption of the residuals.

Returns
-------
testResult : :class:`~openturns.TestResult`
    Test result class.


Notes
-----
We check that the residual is Gaussian thanks to :meth:`~openturns.NormalityTest.AndersonDarlingNormal`."

// ---------------------------------------------------------------------

%feature("docstring") OT::LinearModelAnalysis::getNormalityTestCramerVonMises
"Performs Cramer-Von Mises test.

The statistical test checks the Gaussian assumption of the model (null hypothesis).

Returns
-------
testResult : :class:`~openturns.TestResult`
    Test result class.


Notes
-----
We check that the residual is Gaussian thanks to :meth:`~openturns.NormalityTest.CramerVonMisesNormal`."

// ---------------------------------------------------------------------

%feature("docstring") OT::LinearModelAnalysis::drawModelVsFitted
"Accessor to plot of model versus fitted values.

Returns
-------
graph : :class:`~openturns.Graph`

Notes
-----
The graph plots the sample :math:`(Y_i, \hat{Y}_i)` where :math:`Y_i` is the real value of experience :math:`i`
and :math:`\hat{Y}_i` is the value fitted by the linear model, defined in :eq:`fittedValue` or :eq:`fittedValue2`.
"

// ---------------------------------------------------------------------
%feature("docstring") OT::LinearModelAnalysis::drawResidualsVsFitted
"Accessor to plot of residuals versus fitted values.

Returns
-------
graph : :class:`~openturns.Graph`

Notes
-----
The graph plots the sample :math:`(\varepsilon_i, \hat{Y}_i)` where :math:`\varepsilon_i` is the residual of experience :math:`i` defined in :eq:`residualDef`
and :math:`\hat{Y}_i` is the value fitted by the linear model, defined in :eq:`fittedValue` or :eq:`fittedValue2`.
"

// ---------------------------------------------------------------------

%feature("docstring") OT::LinearModelAnalysis::drawScaleLocation
"Accessor to a Scale-Location plot of sqrt(abs(stand residuals)) versus fitted values.

Returns
-------
graph : :class:`~openturns.Graph`

Notes
-----
The graph plots the sample :math:`(\hat{Y}_i, \sqrt{|\varepsilon_i^{st}|})` where :math:`\varepsilon_i^{st}` is the standardized residual of experience :math:`i` defined in :eq:`stdRes`
and :math:`\hat{Y}_i` is the value fitted by the linear model, defined in :eq:`fittedValue` or :eq:`fittedValue2`.
"

// ---------------------------------------------------------------------

%feature("docstring") OT::LinearModelAnalysis::drawQQplot
"Accessor to plot a Normal quantiles-quantiles plot of standardized residuals.

Returns
-------
graph : :class:`~openturns.Graph`

Notes
-----
The graph plots the empirical quantiles of the standardized residuals defined in :eq:`stdRes` versus
the quantiles of the Normal distribution with zero mean and unit variance.
"

// ---------------------------------------------------------------------

%feature("docstring") OT::LinearModelAnalysis::drawCookDistance
"Accessor to plot of Cook's distances versus row labels.

Returns
-------
graph : :class:`~openturns.Graph`

Notes
-----
The graph plots the Cook distance of each experience :math:`i` is defined in :eq:`cookDef`.
The Cook's distance measures the impact of every individual data point on the linear regression.
"

// ---------------------------------------------------------------------

%feature("docstring") OT::LinearModelAnalysis::drawResidualsVsLeverages
"Accessor to plot of residuals versus leverages that adds bands corresponding to Cook's distances of 0.5 and 1.

Returns
-------
graph : :class:`~openturns.Graph`

Notes
-----
This graph plots the residuals :math:`\varepsilon_i` defined in :eq:`residualDef` and the leverage :math:`\ell_i`  of experience :math:`i`, 
defined as the distance between the point :math:`(\vect{X}^i, Y_i)`
or :math:`(\vect{\phi}(\vect{X}^i), Y_i)` to the linear
regression where :math:`\vect{\phi}(\vect{X}^i) = \Tr{(\phi_1(\vect{X}^i), \dots,  \phi_{p'}(\vect{X}^i))}`.
"

// ---------------------------------------------------------------------

%feature("docstring") OT::LinearModelAnalysis::drawCookVsLeverages
"Accessor to plot of Cook's distances versus leverage/(1-leverage). 

Returns
-------
graph : :class:`~openturns.Graph`

Notes
-----
This graph plots the Cook distance defined in :eq:`cookDef` and the the ration :math:`\ell_i/(1-\ell_i)` where :math:`\ell_i` is the leverage of experience :math:`i`, 
defined as the distance between the point :math:`(\vect{X}^i, Y_i)`
or :math:`(\vect{\phi}(\vect{X}^i), Y_i)` to the linear
regression where :math:`\vect{\phi}(\vect{X}^i) = \Tr{(\phi_1(\vect{X}^i), \dots,  \phi_{p'}(\vect{X}^i))}`.
"

